{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0n49UaWPGAd"
      },
      "source": [
        "# Solve Business Problems with AI\n",
        "\n",
        "## Objective\n",
        "Develop a proof-of-concept application to intelligently process email order requests and customer inquiries for a fashion store. The system should accurately categorize emails as either product inquiries or order requests and generate appropriate responses using the product catalog information and current stock status.\n",
        "\n",
        "You are encouraged to use AI assistants (like ChatGPT or Claude) and any IDE of your choice to develop your solution. Many modern IDEs (such as PyCharm, or Cursor) can work with Jupiter files directly.\n",
        "\n",
        "## Task Description\n",
        "\n",
        "### Inputs\n",
        "\n",
        "Google Spreadsheet **[Document](https://docs.google.com/spreadsheets/d/14fKHsblfqZfWj3iAaM2oA51TlYfQlFT4WKo52fVaQ9U)** containing:\n",
        "\n",
        "- **Products**: List of products with fields including product ID, name, category, stock amount, detailed description, and season.\n",
        "\n",
        "- **Emails**: Sequential list of emails with fields such as email ID, subject, and body.\n",
        "\n",
        "### Instructions\n",
        "\n",
        "- Implement all requirements using advanced Large Language Models (LLMs) to handle complex tasks, process extensive data, and generate accurate outputs effectively.\n",
        "- Use Retrieval-Augmented Generation (RAG) and vector store techniques where applicable to retrieve relevant information and generate responses.\n",
        "- You are provided with a temporary OpenAI API key granting access to GPT-4o, which has a token quota. Use it wisely or use your own key if preferred.\n",
        "- Address the requirements in the order listed. Review them in advance to develop a general implementation plan before starting.\n",
        "- Your deliverables should include:\n",
        "   - Code developed within this notebook.\n",
        "   - A single spreadsheet containing results, organized across separate sheets.\n",
        "   - Comments detailing your thought process.\n",
        "- You may use additional libraries (e.g., langchain) to streamline the solution. Use libraries appropriately to align with best practices for AI and LLM tools.\n",
        "- Use the most suitable AI techniques for each task. Note that solving tasks with traditional programming methods will not earn points, as this assessment evaluates your knowledge of LLM tools and best practices.\n",
        "\n",
        "### Requirements\n",
        "\n",
        "#### 1. Classify emails\n",
        "    \n",
        "Classify each email as either a _**\"product inquiry\"**_ or an _**\"order request\"**_. Ensure that the classification accurately reflects the intent of the email.\n",
        "\n",
        "**Output**: Populate the **email-classification** sheet with columns: email ID, category.\n",
        "\n",
        "#### 2. Process order requests\n",
        "1.   Process orders\n",
        "  - For each order request, verify product availability in stock.\n",
        "  - If the order can be fulfilled, create a new order line with the status “created”.\n",
        "  - If the order cannot be fulfilled due to insufficient stock, create a line with the status “out of stock” and include the requested quantity.\n",
        "  - Update stock levels after processing each order.\n",
        "  - Record each product request from the email.\n",
        "  - **Output**: Populate the **order-status** sheet with columns: email ID, product ID, quantity, status (**_\"created\"_**, **_\"out of stock\"_**).\n",
        "\n",
        "2.   Generate responses\n",
        "  - Create response emails based on the order processing results:\n",
        "      - If the order is fully processed, inform the customer and provide product details.\n",
        "      - If the order cannot be fulfilled or is only partially fulfilled, explain the situation, specify the out-of-stock items, and suggest alternatives or options (e.g., waiting for restock).\n",
        "  - Ensure the email tone is professional and production-ready.\n",
        "  - **Output**: Populate the **order-response** sheet with columns: email ID, response.\n",
        "\n",
        "#### 3. Handle product inquiry\n",
        "\n",
        "Customers may ask general open questions.\n",
        "  - Respond to product inquiries using relevant information from the product catalog.\n",
        "  - Ensure your solution scales to handle a full catalog of over 100,000 products without exceeding token limits. Avoid including the entire catalog in the prompt.\n",
        "  - **Output**: Populate the **inquiry-response** sheet with columns: email ID, response.\n",
        "\n",
        "## Evaluation Criteria\n",
        "- **Advanced AI Techniques**: The system should use Retrieval-Augmented Generation (RAG) and vector store techniques to retrieve relevant information from data sources and use it to respond to customer inquiries.\n",
        "- **Tone Adaptation**: The AI should adapt its tone appropriately based on the context of the customer's inquiry. Responses should be informative and enhance the customer experience.\n",
        "- **Code Completeness**: All functionalities outlined in the requirements must be fully implemented and operational as described.\n",
        "- **Code Quality and Clarity**: The code should be well-organized, with clear logic and a structured approach. It should be easy to understand and maintain.\n",
        "- **Presence of Expected Outputs**: All specified outputs must be correctly generated and saved in the appropriate sheets of the output spreadsheet. Ensure the format of each output matches the requirements—do not add extra columns or sheets.\n",
        "- **Accuracy of Outputs**: The accuracy of the generated outputs is crucial and will significantly impact the evaluation of your submission.\n",
        "\n",
        "We look forward to seeing your solution and your approach to solving real-world problems with AI technologies."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **CANDIDATE**: Himanshu Sharma\n",
        "## **TASK**: Build GenAI-Based Features assessment"
      ],
      "metadata": {
        "id": "oxBLTWr6cwMo"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOUEcKe-xSPr"
      },
      "source": [
        "# Prerequisites"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otSVe-sQ-CsW"
      },
      "source": [
        "### Configure OpenAI API Key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UvOkHAkDghww"
      },
      "outputs": [],
      "source": [
        "# Install the OpenAI Python package.\n",
        "# %pip install openai httpx==0.27.2\n",
        "%pip install faiss-cpu langchain langchain-community tiktoken gspread gspread_dataframe --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0Q0ahvVJdCF9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import faiss\n",
        "import numpy as np\n",
        "from openai import OpenAI\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "from langchain.vectorstores.faiss import FAISS\n",
        "from langchain.docstore.document import Document\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "import re"
      ],
      "metadata": {
        "id": "MGAcz9u9J7n3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**IMPORTANT: If you are going to use our custom API Key then make sure that you also use custom base URL as in example below. Otherwise it will not work.**"
      ],
      "metadata": {
        "id": "IKJf-dPLwXMu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "caEG5zIMw8DV"
      },
      "outputs": [],
      "source": [
        "# ----------------------------- Google Sheets Integration Setup -----------------------------\n",
        "\n",
        "# Import the Google Colab authentication module. This is required to access Google APIs (like Sheets or Drive) securely.\n",
        "from google.colab import auth\n",
        "\n",
        "# Launches an authentication flow in the Colab environment.\n",
        "# The user will be prompted to sign in with their Google account and authorize access.\n",
        "auth.authenticate_user()\n",
        "\n",
        "# Import gspread – a Python API client for Google Sheets.\n",
        "# It allows programmatic access to read, write, and manage Google Sheets.\n",
        "import gspread\n",
        "\n",
        "# Import the default credentials mechanism from Google's OAuth2 client.\n",
        "# This will retrieve the current user's authenticated credentials (after `authenticate_user()`).\n",
        "from google.auth import default\n",
        "\n",
        "# Import helper to write Pandas DataFrames directly to Google Sheets\n",
        "from gspread_dataframe import set_with_dataframe\n",
        "\n",
        "# Retrieve the default credentials and project info (we only need creds here)\n",
        "creds, _ = default()\n",
        "\n",
        "# Create an authorized gspread client using the authenticated credentials.\n",
        "# This object (gc) will be used to create and update Google Sheets programmatically.\n",
        "gc = gspread.authorize(creds)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------- Step 1: Read Product and Email Data from Google Sheets -----------------------------\n",
        "\n",
        "# Define a reusable function to read a specific Google Sheet tab as a DataFrame\n",
        "# The function converts the Google Sheet tab into a downloadable CSV format via Google's Visualization API\n",
        "def read_data_frame(document_id, sheet_name):\n",
        "    url = f\"https://docs.google.com/spreadsheets/d/{document_id}/gviz/tq?tqx=out:csv&sheet={sheet_name}\"\n",
        "    return pd.read_csv(url)  # Read the CSV into a Pandas DataFrame\n",
        "\n",
        "# Google Sheet document ID containing both 'products' and 'emails' sheets\n",
        "document_id = '14fKHsblfqZfWj3iAaM2oA51TlYfQlFT4WKo52fVaQ9U'\n",
        "\n",
        "# Load the product catalog into a DataFrame\n",
        "# This contains fields like product ID, name, description, stock amount, etc.\n",
        "products_df = read_data_frame(document_id, 'products')\n",
        "\n",
        "# Load the list of customer emails into a DataFrame\n",
        "# Each row contains the email ID, subject, and body content\n",
        "emails_df = read_data_frame(document_id, 'emails')\n",
        "\n",
        "\n",
        "# ----------------------------- Step 2: Setup OpenAI Client for GPT-4o Access -----------------------------\n",
        "\n",
        "# Create an OpenAI client instance to interact with GPT-4o via a custom hosted API endpoint\n",
        "client = OpenAI(\n",
        "    base_url='https://47v4us7kyypinfb5lcligtc3x40ygqbs.lambda-url.us-east-1.on.aws/v1/',  # Custom proxy base for OpenAI API\n",
        "    api_key='a0BIj000002c315MAA'  # Temporary API key provided for this coding assessment\n",
        ")"
      ],
      "metadata": {
        "id": "shE97VzGwZ4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zyvqJ-BUNKQR"
      },
      "source": [
        "# Task 1. Classify emails"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------- Step 3: Classify Emails Using GPT-4o -----------------------------\n",
        "\n",
        "# Define a function that uses GPT-4o to classify an email as either a 'product inquiry' or 'order request'\n",
        "def classify_email(subject, body):\n",
        "    # Construct a clear, contextual prompt to instruct the model to classify the email intent\n",
        "    # The model is instructed to respond with only one label to avoid ambiguity\n",
        "    prompt = f\"\"\"You are an intelligent email assistant working for a critical business.\n",
        "Classify this email as either 'product inquiry' or 'order request'.\n",
        "\n",
        "Subject: {subject}\n",
        "Body: {body}\n",
        "\n",
        "Return only one of the two: product inquiry OR order request.\"\"\"\n",
        "\n",
        "    # Send the prompt to GPT-4o via the chat completion endpoint\n",
        "    result = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "    )\n",
        "\n",
        "    # Extract and normalize the response (e.g., remove whitespace and convert to lowercase)\n",
        "    return result.choices[0].message.content.strip().lower()\n",
        "\n",
        "# Initialize a list to store classification results\n",
        "email_classification = []\n",
        "\n",
        "# Iterate over each email in the 'emails_df' DataFrame\n",
        "# For each email, call the classify_email function and append the result to a structured list\n",
        "for _, row in emails_df.iterrows():\n",
        "    category = classify_email(row['subject'], row['message'])  # row['message'] contains the body of the email\n",
        "    email_classification.append({'email_id': row['email_id'], 'category': category})  # Store result with email ID\n",
        "\n",
        "# Convert the classification results into a new DataFrame\n",
        "# This will later be written to the 'email-classification' output sheet\n",
        "email_classification_df = pd.DataFrame(email_classification)"
      ],
      "metadata": {
        "id": "jQoTQyZoLFnS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##The code below is specifically designed to handle a full product catalog containing over 100,000 items without exceeding token limits. It uses a Retrieval-Augmented Generation (RAG) approach powered by FAISS, which ensures that only the most relevant product information is retrieved and passed to the language model, rather than including the entire catalog in the prompt. This makes the solution scalable, efficient, and well-suited for real-world e-commerce scenarios.\n"
      ],
      "metadata": {
        "id": "goQUTECueens"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------- Step 4: Build FAISS Index for Product Inquiry Responses -----------------------------\n",
        "\n",
        "# Construct product documents for the vector database\n",
        "# Each row from the product catalog is converted into a semantic document using LangChain's `Document` class\n",
        "# This document includes a structured natural language description of each product\n",
        "product_docs = [\n",
        "    Document(\n",
        "        page_content=(\n",
        "            f\"Product Name: {row['name']}, \"\n",
        "            f\"Category: {row['category']}, \"\n",
        "            f\"Description: {row['description']}, \"\n",
        "            f\"Season: {row['seasons']}, \"\n",
        "            f\"Stock: {row['stock']}, \"\n",
        "            f\"Price: {row['price']}\"\n",
        "        ),\n",
        "        metadata={\"product ID\": row[\"product_id\"]}\n",
        "    )\n",
        "    for _, row in products_df.iterrows()\n",
        "]\n",
        "\n",
        "# Initialize an OpenAI embedding model to convert product descriptions into dense vector embeddings\n",
        "# This step translates text into high-dimensional numeric vectors that represent semantic meaning\n",
        "embedding_model = OpenAIEmbeddings(\n",
        "    openai_api_key='a0BIj000002c315MAA',\n",
        "    openai_api_base=\"https://47v4us7kyypinfb5lcligtc3x40ygqbs.lambda-url.us-east-1.on.aws/v1/\"  # Custom API proxy for GPT-4o\n",
        ")\n",
        "\n",
        "# Use FAISS to build a fast vector store from the embedded product documents\n",
        "# FAISS enables similarity search, allowing us to retrieve the top-K most relevant products for a query\n",
        "vectorstore = FAISS.from_documents(product_docs, embedding_model)\n",
        "\n",
        "# Convert FAISS index into a retriever object\n",
        "# The retriever will fetch top 5 semantically similar products for any product inquiry\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
        "\n",
        "# Create a Retrieval-Augmented Generation (RAG) QA chain using LangChain\n",
        "# This integrates GPT-4o with the retriever so that only relevant product information is passed to the LLM\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=ChatOpenAI(\n",
        "        openai_api_key='a0BIj000002c315MAA',\n",
        "        model=\"gpt-4o\",\n",
        "        openai_api_base=\"https://47v4us7kyypinfb5lcligtc3x40ygqbs.lambda-url.us-east-1.on.aws/v1/\"\n",
        "    ),\n",
        "    retriever=retriever\n",
        ")\n"
      ],
      "metadata": {
        "id": "wnF7_u2dMAeR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------- FAISS-based Product Name Matching -----------------------------\n",
        "\n",
        "def match_product_by_name(product_name_query):\n",
        "    \"\"\"\n",
        "    Uses FAISS vector store to find the best matching product from the catalog\n",
        "    based on a free-text name or description returned by the LLM.\n",
        "\n",
        "    This function is crucial when the LLM output does not exactly match the product\n",
        "    names in the catalog. Instead of relying on strict string matching, we leverage\n",
        "    semantic search using embeddings to handle variations, typos, and fuzzy phrasing.\n",
        "    \"\"\"\n",
        "\n",
        "    # Perform a similarity search on the FAISS vector index using the LLM's output as the query.\n",
        "    # The result is a ranked list of product documents (wrapped as LangChain Document objects),\n",
        "    # ordered by how semantically close they are to the given input.\n",
        "    docs = vectorstore.similarity_search(product_name_query, k=1)  # Get top-1 match\n",
        "\n",
        "    # In rare cases where no similar product is found, we return None gracefully.\n",
        "    if not docs:\n",
        "        return None\n",
        "\n",
        "    # Extract the metadata field 'product ID' from the best matching document.\n",
        "    # This ID was attached to the FAISS index when we constructed it earlier from the product catalog.\n",
        "    matched_product_id = docs[0].metadata['product ID']\n",
        "\n",
        "    # Retrieve and return the full product row from the original product DataFrame using the matched ID.\n",
        "    # This allows the rest of the pipeline (e.g., stock checking, fulfillment) to work with structured product data.\n",
        "    return products_df[products_df['product_id'] == matched_product_id].iloc[0]\n"
      ],
      "metadata": {
        "id": "N2OMPNjawPpV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lm3ZeatjYjM3"
      },
      "source": [
        "# Task 2. Process order requests"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------- Step 5: Extract and Fulfill Orders (Supports Partial Fulfillment) -----------------------------\n",
        "\n",
        "# Initialize lists to store structured output for order statuses and email responses\n",
        "order_status_records = []     # Will hold product-level order processing results\n",
        "order_responses = []          # Will hold AI-generated customer email replies\n",
        "\n",
        "# Create a modifiable dictionary of stock levels indexed by product_id\n",
        "# This will be updated in real time as orders are processed\n",
        "updated_stock = products_df.set_index(\"product_id\")['stock'].to_dict()\n",
        "\n",
        "# ----------------------------- Helper Function: Extract Order Info from Email Using GPT-4o -----------------------------\n",
        "\n",
        "def extract_order_info(email_text):\n",
        "    \"\"\"\n",
        "    Uses GPT-4o to parse unstructured order emails and extract product names and quantities.\n",
        "    A special rule is included: if the customer says they want 'all' stock, GPT will return -999999.\n",
        "    \"\"\"\n",
        "    prompt = f\"\"\"You are a professional customer support agent working for a critical business.\n",
        "Extract product name and quantity from this order request email.\n",
        "\n",
        "Email: {email_text}\n",
        "\n",
        "Return as a list in format: product_name ### quantity\n",
        "(e.g., T-shirt ### 2). The quantity should always be a number. If the customer gives a range like 3 to 4, or 3-4, return quantity as lower limit i.e. 3.\n",
        "If a customer wants all available stock, return quantity as '-999999'.\"\"\"\n",
        "\n",
        "    result = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "    )\n",
        "    return result.choices[0].message.content.strip().split(\"\\n\")\n",
        "\n",
        "# ----------------------------- Main Loop: Process Each Email -----------------------------\n",
        "\n",
        "for _, row in emails_df.iterrows():\n",
        "    email_id, subject, body = row[\"email_id\"], row[\"subject\"], row[\"message\"]\n",
        "\n",
        "    # Retrieve the classification category ('order request' or 'product inquiry')\n",
        "    category = email_classification_df.loc[\n",
        "        email_classification_df['email_id'] == email_id,\n",
        "        'category'\n",
        "    ].values[0]\n",
        "\n",
        "    # Process only order request emails\n",
        "    if category == \"order request\":\n",
        "        order_lines = extract_order_info(body)  # Use GPT to extract order lines\n",
        "        order_summary = \"\"  # To accumulate response text for the customer\n",
        "\n",
        "        for line in order_lines:\n",
        "            if '###' not in line:\n",
        "                continue  # Skip lines not matching expected format\n",
        "\n",
        "            # Split line into product name and quantity\n",
        "            product_name, quantity = line.split('###')\n",
        "            # Remove any non-numeral characters from quantity using regular expressions\n",
        "            quantity = re.sub(r\"[^0-9-]\", \"\", quantity.strip())\n",
        "\n",
        "            # Check if quantity is empty after removing non-numeral characters\n",
        "            if quantity:\n",
        "                quantity = int(quantity)\n",
        "            else:\n",
        "                # Handle the case where quantity is empty (e.g., set to 0 or skip)\n",
        "                # You might want to log this scenario or provide a default value\n",
        "                quantity = 0  # Example: Setting quantity to 0\n",
        "\n",
        "            product_name = product_name.strip()\n",
        "\n",
        "            # Match product by FAISS\n",
        "            try:\n",
        "                matched_product = match_product_by_name(product_name)\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "            product_id = matched_product['product_id']\n",
        "            stock = updated_stock.get(product_id, 0)\n",
        "\n",
        "            # Determine how much can be fulfilled\n",
        "            if quantity == -999999 or stock >= quantity:\n",
        "                # Case: all stock requested or enough stock available\n",
        "                status = \"created\"\n",
        "                fulfilled_qty = stock if quantity == -999999 else quantity\n",
        "                updated_stock[product_id] -= fulfilled_qty\n",
        "            elif stock > 0:\n",
        "                # Case: only partial stock available\n",
        "                status = \"partially fulfilled\"\n",
        "                fulfilled_qty = stock\n",
        "                updated_stock[product_id] = 0\n",
        "            else:\n",
        "                # Case: no stock available\n",
        "                status = \"out of stock\"\n",
        "                fulfilled_qty = 0\n",
        "\n",
        "            # Record the outcome of this order line\n",
        "            order_status_records.append({\n",
        "                \"email ID\": email_id,\n",
        "                \"product ID\": product_id,\n",
        "                \"quantity\": quantity,\n",
        "                \"status\": status\n",
        "            })\n",
        "\n",
        "            # Add line to order summary for email generation\n",
        "            order_summary += (\n",
        "                f\"{product_name} - Requested: {quantity}, \"\n",
        "                f\"Fulfilled: {fulfilled_qty}, Status: {status}\\n\"\n",
        "            )\n",
        "\n",
        "        # ----------------------------- Generate Customer-Facing Email Using GPT-4o -----------------------------\n",
        "\n",
        "        response_prompt = f\"\"\"You are a professional customer support agent working for a critical business.\n",
        "Below is the order summary for a customer:\n",
        "\n",
        "{order_summary}\n",
        "\n",
        "Write a polite and professional email informing them of the order status.\n",
        "If any item is partially or not fulfilled, explain why and offer alternatives or suggest restocking options.\"\"\"\n",
        "\n",
        "        reply = client.chat.completions.create(\n",
        "            model=\"gpt-4o\",\n",
        "            messages=[{\"role\": \"user\", \"content\": response_prompt}]\n",
        "        )\n",
        "\n",
        "        # Store the generated response tied to the email ID\n",
        "        order_responses.append({\n",
        "            \"email_id\": email_id,\n",
        "            \"response\": reply.choices[0].message.content.strip()\n",
        "        })"
      ],
      "metadata": {
        "id": "rLVoKYuoPLYa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29ExT_MQRhos"
      },
      "source": [
        "# Task 3. Handle product inquiry"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------- Step 6: Respond to Product Inquiries using RAG -----------------------------\n",
        "\n",
        "# Initialize a list to store AI-generated responses to product inquiry emails\n",
        "inquiry_responses = []\n",
        "\n",
        "# Loop through each email in the dataset\n",
        "for _, row in emails_df.iterrows():\n",
        "    email_id = row['email_id']  # Unique identifier for the email\n",
        "\n",
        "    # Look up the classification of the current email (e.g., 'product inquiry' or 'order request')\n",
        "    category = email_classification_df.loc[\n",
        "        email_classification_df['email_id'] == email_id,\n",
        "        'category'\n",
        "    ].values[0]\n",
        "\n",
        "    # Only process emails that have been classified as product inquiries\n",
        "    if category == \"product inquiry\":\n",
        "        # Use the Retrieval-Augmented Generation (RAG) pipeline to answer the question\n",
        "        # The qa_chain retrieves the top 5 relevant products using FAISS, then feeds them into GPT-4o\n",
        "        result = qa_chain.run(row['message'])\n",
        "\n",
        "        # Store the response along with the email ID in a structured format\n",
        "        inquiry_responses.append({\n",
        "            \"email_id\": email_id,\n",
        "            \"response\": result.strip()\n",
        "        })"
      ],
      "metadata": {
        "id": "1M8zMax7URsJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 4. Write All Outputs to Google Spreadsheet"
      ],
      "metadata": {
        "id": "m1Foc96GWBzI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------------- Step 7: Write All Outputs to Google Spreadsheet -----------------------------\n",
        "\n",
        "# Create a new Google Spreadsheet document named as per the assessment requirement\n",
        "# This will serve as the final submission artifact containing all categorized outputs\n",
        "# Check if the spreadsheet already exists\n",
        "try:\n",
        "    output_document = gc.open('Solving Business Problems with AI - Output___')\n",
        "    print(\"Spreadsheet found. Updating existing spreadsheet.\")\n",
        "except gspread.exceptions.SpreadsheetNotFound:\n",
        "    # Create a new Google Spreadsheet document named as per the assessment requirement\n",
        "    # This will serve as the final submission artifact containing all categorized outputs\n",
        "    output_document = gc.create('Solving Business Problems with AI - Output')\n",
        "    print(\"Spreadsheet not found. Creating a new spreadsheet.\")\n",
        "\n",
        "# ----------------------------- Sheet 1: Email Classification -----------------------------\n",
        "\n",
        "# Add a new sheet for email classification results\n",
        "sheet1 = output_document.add_worksheet(title=\"email-classification\", rows=100, cols=2)\n",
        "\n",
        "# Set the header for the classification sheet\n",
        "sheet1.update([['email ID', 'category']], 'A1:B1')\n",
        "\n",
        "# Write the classification DataFrame to the sheet\n",
        "set_with_dataframe(sheet1, email_classification_df)\n",
        "\n",
        "# ----------------------------- Sheet 2: Order Status -----------------------------\n",
        "\n",
        "# Convert recorded order line statuses into a DataFrame\n",
        "order_status_df = pd.DataFrame(order_status_records)\n",
        "\n",
        "# Add a new sheet for order processing status\n",
        "sheet2 = output_document.add_worksheet(title=\"order-status\", rows=100, cols=4)\n",
        "\n",
        "# Set the header for the order status sheet\n",
        "sheet2.update([['email ID', 'product ID', 'quantity', 'status']], 'A1:D1')\n",
        "\n",
        "# Write the order status DataFrame to the sheet\n",
        "set_with_dataframe(sheet2, order_status_df)\n",
        "\n",
        "# ----------------------------- Sheet 3: Order Responses -----------------------------\n",
        "\n",
        "# Convert order email responses into a DataFrame\n",
        "order_response_df = pd.DataFrame(order_responses)\n",
        "\n",
        "# Add a sheet for GPT-4o generated responses to order requests\n",
        "sheet3 = output_document.add_worksheet(title=\"order-response\", rows=100, cols=2)\n",
        "\n",
        "# Set the header for the order response sheet\n",
        "sheet3.update([['email ID', 'response']], 'A1:B1')\n",
        "\n",
        "# Write the order response DataFrame to the sheet\n",
        "set_with_dataframe(sheet3, order_response_df)\n",
        "\n",
        "# ----------------------------- Sheet 4: Inquiry Responses -----------------------------\n",
        "\n",
        "# Convert inquiry email responses into a DataFrame\n",
        "inquiry_response_df = pd.DataFrame(inquiry_responses)\n",
        "\n",
        "# Add a sheet for GPT-4o generated responses to product inquiries\n",
        "sheet4 = output_document.add_worksheet(title=\"inquiry-response\", rows=100, cols=2)\n",
        "\n",
        "# Set the header for the inquiry response sheet\n",
        "sheet4.update([['email ID', 'response']], 'A1:B1')\n",
        "\n",
        "# Write the inquiry response DataFrame to the sheet\n",
        "set_with_dataframe(sheet4, inquiry_response_df)\n",
        "\n",
        "# ----------------------------- Make Spreadsheet Publicly Accessible -----------------------------\n",
        "\n",
        "# Share the Google Spreadsheet with anyone who has the link (read-only access)\n",
        "output_document.share('', perm_type='anyone', role='reader')\n",
        "\n",
        "# Print the shareable link to access the completed output document\n",
        "print(f\"\\n✅ All tasks completed successfully. The Google sheet can be accessed here:\\nhttps://docs.google.com/spreadsheets/d/{output_document.id}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "65EGsejjWFPf",
        "outputId": "0f46ea14-8d36-4aae-9676-d357513cb6fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spreadsheet not found. Creating a new spreadsheet.\n",
            "\n",
            "✅ All tasks completed successfully. The Google sheet can be accessed here:\n",
            "https://docs.google.com/spreadsheets/d/1TxVsgksB2NGfRygQxBnaQVQrgVzDd9pu-grwtCEI_bU\n"
          ]
        }
      ]
    }
  ]
}